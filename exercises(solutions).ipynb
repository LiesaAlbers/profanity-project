{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6680423b-eefc-445e-aae1-a6db1cbc72c9",
   "metadata": {},
   "source": [
    "## 3. Active Learning Exercises: Calculating Cohen's Kappa in Python Solutions\n",
    "\n",
    "In our analysis we calculated inter-annotator agreement using Python's Cohen's kappa function from the `Scikit` library. However, this statistic can of course also be calculated manually. In this excerise we will teach you how to compute Cohen's Kappa manually by going through the calculations step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bace5-d4e1-47b6-8ad7-b0c65a2b8adf",
   "metadata": {},
   "source": [
    "### 3.1. Understanding the Confusion Matrix\n",
    "\n",
    "Cohen's Kappa is based on a confusion matrix, a table that reports the number of true positives, false negatives, false positives, and true negatives.\n",
    "\n",
    "Let's assume we have two annotators, Annotators A and B, who classified profanity in 100 lines of lyrics into two categories: \"Positive\" and \"Negative\":\n",
    "\n",
    "| A |  B | Count |\n",
    "|---------|---------|-------|\n",
    "| Positive| Positive| 50    |\n",
    "| Positive| Negative| 10    |\n",
    "| Negative| Positive| 5     |\n",
    "| Negative| Negative| 35    |\n",
    "\n",
    "In this table:\n",
    "- The first row indicates that both annotators classified 50 profanity occurences as \"Positive\".\n",
    "- The second row indicates that Annotator A classified 10 profanity occurences as \"Positive\" while Annotator B classified them as \"Negative\".\n",
    "- The third row indicates that Annotator A classified 5 profanity occurences as \"Negative\" while Annotator B classified them as \"Positive\".\n",
    "- The fourth row indicates that both Annotators classified 35 profanity occurences as \"Negative\".\n",
    "\n",
    "Your first exercise is to create a confusion matrix based on the example data above using the `NumPy` package, which is a fundamental package for numerical computing in Python. It provides support for arrays and mathematical functions.\n",
    "\n",
    "*(Hint: you'll need the `array` function, see documentation [here](https://numpy.org/doc/2.1/reference/generated/numpy.array.html))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "32b1c348-d952-40fe-b463-929a210400f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50 10]\n",
      " [ 5 35]]\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy:\n",
    "import numpy as np\n",
    "\n",
    "# Create the confusion matrix:\n",
    "confusion_matrix = np.array([[50, 10],\n",
    "                              [5, 35]])\n",
    "# Print the matrix:\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004c27f-8d01-4a25-a7a7-776b1d3d76bf",
   "metadata": {},
   "source": [
    "### 3.2. Calculate Observed Agreement (Po)\n",
    "\n",
    "Next, we will need to calculate the observed agreement (Po), the proportion of times the annotators agree on their classifications. The cases where the two annotators agree are visible in the *diagonal* of the confusion matrix (top left to bottom right). \n",
    "\n",
    "The formula for observed agreement is:\n",
    "\n",
    "$P_o = \\frac{( a  +  d )}{N}$ \n",
    "\n",
    "Where:\n",
    "- a = number of agreements in the positive category (top-left cell in the confusion matrix)\n",
    "- d = number of agreements in the negative category (bottom-right cell in the confusion matrix)\n",
    "- N = total number of observations (sum of all cells in the confusion matrix)\n",
    "\n",
    "Your task is to calculate Po using the confusion matrix and formula. Index the confusion matrix to extract the necessary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "86c33499-3a6d-42fa-953d-d534e00bfc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Agreement = 0.85\n"
     ]
    }
   ],
   "source": [
    "# Use indexing to extract the values ('a' and 'd') needed for the calculation of Po:\n",
    "a = confusion_matrix[0, 0]  # Positive-Positive\n",
    "d = confusion_matrix[1, 1]  # Negative-Negative\n",
    "\n",
    "# Calculate the total number of observations (N) by summing all the elements in the confusion matrix:\n",
    "N = np.sum(confusion_matrix)\n",
    "\n",
    "# Calculate Po using the formula:\n",
    "P_o = (a + d) / N\n",
    "\n",
    "# Print the result:\n",
    "print(f'Observed Agreement =', P_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1f89e-49db-4e15-b51d-bf843a84ae9f",
   "metadata": {},
   "source": [
    "### 3.3. Calculate Expected Agreement (Pe)\n",
    "\n",
    "We will now do the same for the expected agreement (Pe), the agreement between two annotators that would be expected by chance. \n",
    "\n",
    "The formula for expected agreement is:\n",
    "\n",
    "$P_e = \\left(\\frac{(a+b)(a+c)}{N^2}\\right) + \\left(\\frac{(c+d)(b+d)}{N^2}\\right)$ \n",
    "\n",
    "Where:\n",
    "- b = number of positive ratings by Annotator A and negative ratings by Annotator B (top-right cell in the confusion matrix)\n",
    "- c = number of negative ratings by Annotator A and positive ratings by Annotator B (bottom-left cell in the confusion matrix)\n",
    "\n",
    "*(Note that you already created variables for values `a`, `d` and `N` in the previous exercise)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a23267f1-5632-4849-9027-77ebe0aaad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Agreement = 0.51\n"
     ]
    }
   ],
   "source": [
    "# Use indexing to extract the values ('b' and 'c') needed for the calculation of Pe:\n",
    "b = confusion_matrix[0, 1]  # Positive-Negative\n",
    "c = confusion_matrix[1, 0]  # Negative-Positive\n",
    "\n",
    "# Calculate Pe using the formula:\n",
    "P_e = ((a + b) * (a + c) + (c + d) * (b + d)) / (N ** 2)\n",
    "\n",
    "# Print the result:\n",
    "print(f'Expected Agreement =', P_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724f98e-de74-465b-a27b-4e9eefdbe5d9",
   "metadata": {},
   "source": [
    "### 3.4. Calculate Cohen's Kappa ($\\kappa$)\n",
    "\n",
    "Now we have the values needed to calculate Cohen's Kappa ($\\kappa$). \n",
    "\n",
    "Calculate $\\kappa$ with its formula:\n",
    "\n",
    "$ \\kappa = \\frac{(P_o - P_e)} {(1 - P_e)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc7374a9-3d88-4b40-9dec-23be6f8cfb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa = 0.6938775510204082\n"
     ]
    }
   ],
   "source": [
    "# Calculate k using the formula:\n",
    "k = (P_o - P_e) / (1 - P_e)\n",
    "\n",
    "# Print the result:\n",
    "print(f\"Cohen's Kappa =\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c722d-e567-4e31-8329-07bd150a2f2d",
   "metadata": {},
   "source": [
    "### 3.5. Interpreting Cohen's Kappa\n",
    "\n",
    "After calculating Cohen's Kappa ($\\kappa$), you will have a numerical value that falls between -1 and 1, that indicates the level of agreement between the two annotators. But, we would of course like to know what this numerical value actually *means*.\n",
    "\n",
    "Consulting [this](https://datatab.net/tutorial/cohens-kappa) documentation, write a few sentences below interpreting the $\\kappa$ value you just computed:\n",
    "\n",
    "    (Assuming you calculated Cohen's Kappa and obtained a value of 0.69)\n",
    "\n",
    "    The Cohen's Kappa of 0.69: falls within Landis & Koch's (1977) range of 0.61 - 0.80, indicating substantial agreement between the two annotators. This suggests that the annotators are generally consistent in their classifications, but there may still be some discrepancies that could be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8fb97-69a2-432f-8845-debc1a4f8ec7",
   "metadata": {},
   "source": [
    "### 3.6. Reflection\n",
    "\n",
    "We now know that Cohen's Kappa is a statistical measure used to assess inter-annotator reliability in classification tasks. While high agreement isn't absolutely critical for all applications, such as with our data example of analysing profanity sentiment in music lyrics, there are many fields where consistent classification is crucial and it can never hurt to include the measure in your research. \n",
    "\n",
    "Can you identify any fields/domains where achieving a high Cohen's Kappa is particularly important?\n",
    "\n",
    "    Consistent classification is important in any field that requires subjective judgement. Some fields where it is particularly important are medical diagnosis and psychological assesments. Inconsistent diagnoses could lead to misdiagnosis, delayed treatment, or missed critical conditions, potentially putting patients' lives at risk. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
